{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Install system dependencies for Box2D and Rendering (Headless)\n!sudo apt-get update -y\n!sudo apt-get install -y swig xvfb python3-opengl ffmpeg\n\n# 2. Install Python libraries\n# pyvirtualdisplay is crucial for rendering on Kaggle\n!pip install gymnasium[box2d] moviepy imageio wandb huggingface_hub pyvirtualdisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T20:04:28.699040Z","iopub.execute_input":"2025-12-10T20:04:28.699721Z","iopub.status.idle":"2025-12-10T20:05:39.032544Z","shell.execute_reply.started":"2025-12-10T20:04:28.699692Z","shell.execute_reply":"2025-12-10T20:05:39.031752Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease    \nHit:7 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,087 kB]\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,201 kB]\nGet:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\nGet:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,575 kB]\nGet:13 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\nGet:14 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]          \nGet:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,918 kB]\nGet:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,519 kB] \nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,327 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\nGet:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,844 kB]\nFetched 37.9 MB in 3s (13.6 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\nxvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\nThe following additional packages will be installed:\n  freeglut3 libglu1-mesa swig4.0\nSuggested packages:\n  libgle3 python3-numpy swig-doc swig-examples swig4.0-examples swig4.0-doc\nThe following NEW packages will be installed:\n  freeglut3 libglu1-mesa python3-opengl swig swig4.0\n0 upgraded, 5 newly installed, 0 to remove and 187 not upgraded.\nNeed to get 1,940 kB of archives.\nAfter this operation, 13.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\nFetched 1,940 kB in 0s (6,646 kB/s)\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package freeglut3:amd64.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\nUnpacking freeglut3:amd64 (2.8.1-6) ...\nSelecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\nUnpacking libglu1-mesa:amd64 (9.0.2-1) ...\nSelecting previously unselected package python3-opengl.\nPreparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\nUnpacking python3-opengl (3.1.5+dfsg-1) ...\nSelecting previously unselected package swig4.0.\nPreparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\nUnpacking swig4.0 (4.0.2-1ubuntu1) ...\nSelecting previously unselected package swig.\nPreparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\nUnpacking swig (4.0.2-1ubuntu1) ...\nSetting up freeglut3:amd64 (2.8.1-6) ...\nSetting up libglu1-mesa:amd64 (9.0.2-1) ...\nSetting up swig4.0 (4.0.2-1ubuntu1) ...\nSetting up swig (4.0.2-1ubuntu1) ...\nSetting up python3-opengl (3.1.5+dfsg-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nProcessing triggers for man-db (2.10.2-1) ...\nRequirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.36.0)\nCollecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\nRequirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\nCollecting box2d-py==2.3.5 (from gymnasium[box2d])\n  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\nCollecting swig==4.* (from gymnasium[box2d])\n  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.5)\nRequirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.12)\nRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.3.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.33.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.2.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.10.5)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[box2d]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nDownloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nBuilding wheels for collected packages: box2d-py\n  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351232 sha256=5d480d6af153983bc0dba162661d19f6a56591920d8c40c88b9e7206a466b1db\n  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\nSuccessfully built box2d-py\nInstalling collected packages: swig, pyvirtualdisplay, box2d-py\nSuccessfully installed box2d-py-2.3.5 pyvirtualdisplay-3.0 swig-4.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import subprocess\nimport sys\n\n# Upgrade gymnasium to latest version\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gymnasium\", \"--upgrade\"])\n!pip install swig\n!pip install \"gymnasium[box2d]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T20:05:39.034153Z","iopub.execute_input":"2025-12-10T20:05:39.034626Z","iopub.status.idle":"2025-12-10T20:05:49.249766Z","shell.execute_reply.started":"2025-12-10T20:05:39.034600Z","shell.execute_reply":"2025-12-10T20:05:49.249021Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (0.29.0)\nCollecting gymnasium\n  Downloading gymnasium-1.2.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\nDownloading gymnasium-1.2.2-py3-none-any.whl (952 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 952.1/952.1 kB 26.1 MB/s eta 0:00:00\nInstalling collected packages: gymnasium\n  Attempting uninstall: gymnasium\n    Found existing installation: gymnasium 0.29.0\n    Uninstalling gymnasium-0.29.0:\n      Successfully uninstalled gymnasium-0.29.0\nSuccessfully installed gymnasium-1.2.2\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.2.2 which is incompatible.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.2.2 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\nRequirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\nRequirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[box2d]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport uuid\nfrom dataclasses import dataclass, asdict\nfrom typing import Optional, Tuple\n\n# --- KAGGLE SPECIFIC: START VIRTUAL DISPLAY ---\nfrom pyvirtualdisplay import Display\ntry:\n    # Create a virtual screen to trick OpenGL\n    display = Display(visible=0, size=(1400, 900))\n    display.start()\n    print(\"Virtual display started successfully.\")\nexcept Exception as e:\n    print(f\"Failed to start virtual display: {e}\")\n# ---------------------------------------------\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport wandb\nfrom huggingface_hub import HfApi, create_repo, upload_folder\nfrom kaggle_secrets import UserSecretsClient \n\n# ==========================================\n# 0. Setup WandB Login for Kaggle\n# ==========================================\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n    wandb.login(key=wandb_api_key)\n    print(\"Logged into WandB via Kaggle Secrets.\")\nexcept:\n    print(\"Could not find 'wandb_api_key' in Kaggle Secrets. Falling back to interactive login.\")\n    wandb.login()\n\n# ==========================================\n# 1. Configuration & Hyperparameters (SAC)\n# ==========================================\n@dataclass\nclass SACConfig:\n    # Experiment Settings\n    env_id: str = \"CarRacing-v3\"\n    project_name: str = \"sac-carracing16\"\n    run_name: str = f\"sac_car_{str(uuid.uuid4())[:8]}\"\n    seed: int = 42\n    \n    # Training Duration \n    total_timesteps: int = 500_000  \n    # warm up steps (heuristic agent)\n    learning_starts: int = 20_000        \n    \n    # Hyperparameters\n    hidden_dim: int = 256\n    actor_lr: float = 3e-4  \n    critic_lr: float = 3e-4\n    batch_size: int = 256     # SAC typically benefits from larger batches\n    buffer_size: int = 100_000 \n    gamma: float = 0.99                \n    tau: float = 0.005                \n    \n    # SAC Specifics\n    alpha: float = 0.2        # Entropy regularization coefficient\n    target_entropy: float = -3.0 # (Optional if we implemented auto-tune, but we stick to fixed alpha per image)\n\n    # Logging & Saving\n    eval_freq: int = 10_000\n    save_model: bool = True\n    hf_repo_id: str = \"yousefyousefyousef335/sac-carracing-v3\"\n\n# ==========================================\n# 2. Preprocessing & Wrappers\n# ==========================================\nclass ImageTransposeWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255, \n            shape=(obs_shape[2], obs_shape[0], obs_shape[1]), \n            dtype=np.uint8\n        )\n\n    def observation(self, observation):\n        return np.transpose(observation, (2, 0, 1))\n\n# ==========================================\n# 3. Replay Buffer (Optimized for Images)\n# ==========================================\nclass ReplayBuffer:\n    def __init__(self, state_shape, action_dim, max_size=1e5):\n        self.max_size = int(max_size)\n        self.ptr = 0\n        self.size = 0\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.state = np.zeros((self.max_size, *state_shape), dtype=np.uint8)\n        self.action = np.zeros((self.max_size, action_dim), dtype=np.float32)\n        self.next_state = np.zeros((self.max_size, *state_shape), dtype=np.uint8)\n        self.reward = np.zeros((self.max_size, 1), dtype=np.float32)\n        self.not_done = np.zeros((self.max_size, 1), dtype=np.float32)\n\n    def add(self, state, action, next_state, reward, done):\n        self.state[self.ptr] = state\n        self.action[self.ptr] = action\n        self.next_state[self.ptr] = next_state\n        self.reward[self.ptr] = reward\n        self.not_done[self.ptr] = 1. - done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, self.size, size=batch_size)\n        \n        return (\n            torch.FloatTensor(self.state[ind]).to(self.device) / 255.0,\n            torch.FloatTensor(self.action[ind]).to(self.device),\n            torch.FloatTensor(self.next_state[ind]).to(self.device) / 255.0,\n            torch.FloatTensor(self.reward[ind]).to(self.device),\n            torch.FloatTensor(self.not_done[ind]).to(self.device)\n        )\n\n# ==========================================\n# 4. Neural Networks (CNN + MLP)\n# ==========================================\nclass CNNEncoder(nn.Module):\n    def __init__(self, input_channels=3):\n        super(CNNEncoder, self).__init__()\n        # Input: (3, 96, 96)\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4), # -> (32, 23, 23)\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),             # -> (64, 10, 10)\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),             # -> (64, 8, 8)\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        self.out_dim = 64 * 8 * 8 \n\n    def forward(self, x):\n        return self.net(x)\n\nclass GaussianActor(nn.Module):\n    def __init__(self, action_dim, max_action, hidden_dim=256):\n        super(GaussianActor, self).__init__()\n        self.encoder = CNNEncoder()\n        \n        self.l1 = nn.Linear(self.encoder.out_dim, hidden_dim)\n        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # SAC outputs Mean and Log-Std\n        self.mu_layer = nn.Linear(hidden_dim, action_dim)\n        self.log_std_layer = nn.Linear(hidden_dim, action_dim)\n        \n        self.max_action = max_action\n        self.LOG_STD_MAX = 2\n        self.LOG_STD_MIN = -20\n\n    def forward(self, state):\n        features = self.encoder(state)\n        a = F.relu(self.l1(features))\n        a = F.relu(self.l2(a))\n        \n        mu = self.mu_layer(a)\n        log_std = self.log_std_layer(a)\n        \n        # Clamp log_std to maintain numerical stability\n        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n        return mu, log_std\n\n    def sample(self, state):\n        mu, log_std = self.forward(state)\n        std = log_std.exp()\n        \n        # Reparameterization Trick: a = mu + std * epsilon\n        normal = torch.distributions.Normal(mu, std)\n        x_t = normal.rsample()  # for reparameterization gradient\n        y_t = torch.tanh(x_t)   # Squash to [-1, 1]\n        \n        action = y_t * self.max_action\n        \n        # Enforcing Action Bound (Log Prob Correction for Tanh)\n        # log_prob = log_prob_normal - sum(log(1 - tanh(x)^2))\n        log_prob = normal.log_prob(x_t)\n        # 1e-6 for numerical stability\n        log_prob -= torch.log(self.max_action * (1 - y_t.pow(2)) + 1e-6)\n        log_prob = log_prob.sum(1, keepdim=True)\n        \n        return action, log_prob\n\n    def select_action(self, state):\n        # Deterministic action for evaluation (just the mean squashed)\n        with torch.no_grad():\n            mu, _ = self.forward(state)\n            action = torch.tanh(mu) * self.max_action\n        return action.cpu().data.numpy().flatten()\n\nclass Critic(nn.Module):\n    def __init__(self, action_dim, hidden_dim=256):\n        super(Critic, self).__init__()\n        self.encoder1 = CNNEncoder()\n        self.encoder2 = CNNEncoder()\n\n        # Q1 Architecture\n        self.l1 = nn.Linear(self.encoder1.out_dim + action_dim, hidden_dim)\n        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n        self.l3 = nn.Linear(hidden_dim, 1)\n\n        # Q2 Architecture\n        self.l4 = nn.Linear(self.encoder2.out_dim + action_dim, hidden_dim)\n        self.l5 = nn.Linear(hidden_dim, hidden_dim)\n        self.l6 = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state, action):\n        f1 = self.encoder1(state)\n        sa1 = torch.cat([f1, action], 1)\n        q1 = F.relu(self.l1(sa1))\n        q1 = F.relu(self.l2(q1))\n        q1 = self.l3(q1)\n\n        f2 = self.encoder2(state)\n        sa2 = torch.cat([f2, action], 1)\n        q2 = F.relu(self.l4(sa2))\n        q2 = F.relu(self.l5(q2))\n        q2 = self.l6(q2)\n        return q1, q2\n\n# ==========================================\n# 5. Soft Actor-Critic (SAC) Algorithm\n# ==========================================\nclass SAC:\n    def __init__(self, action_dim, max_action, config: SACConfig):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.conf = config\n        self.max_action = max_action\n\n        # Actor: Outputs (Mean, Log_Std)\n        self.actor = GaussianActor(action_dim, max_action, config.hidden_dim).to(self.device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)\n\n        # Critic: Two Q-functions\n        self.critic = Critic(action_dim, config.hidden_dim).to(self.device)\n        self.critic_target = Critic(action_dim, config.hidden_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)\n        \n        # Entropy Coefficient\n        self.alpha = config.alpha\n\n    def select_action(self, state, evaluate=False):\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0\n        if evaluate:\n            return self.actor.select_action(state)\n        else:\n            action, _ = self.actor.sample(state)\n            return action.cpu().data.numpy().flatten()\n\n    def train(self, replay_buffer):\n        # Sample batch\n        state, action, next_state, reward, not_done = replay_buffer.sample(self.conf.batch_size)\n\n        # -------------------------------------\n        # 1. Critic Update\n        # -------------------------------------\n        with torch.no_grad():\n            # Sample next action from CURRENT policy (not target policy like TD3)\n            # This follows the Algorithm 1 logic: a' ~ pi(.|s')\n            next_action, next_log_prob = self.actor.sample(next_state)\n\n            # Compute Target Q\n            # y = r + gamma * (min(Q_targ1, Q_targ2) - alpha * log_pi)\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n            target_Q = torch.min(target_Q1, target_Q2) - self.alpha * next_log_prob\n            target_Q = reward + not_done * self.conf.gamma * target_Q\n\n        # Current Q\n        current_Q1, current_Q2 = self.critic(state, action)\n        \n        # Critic Loss (MSE)\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # -------------------------------------\n        # 2. Actor Update\n        # -------------------------------------\n        # Sample action from current policy for current state\n        # Algorithm 1: Update phi by one step of gradient ascent using:\n        # min(Q(s, a_tilde)) - alpha * log_pi(a_tilde | s)\n        new_action, log_prob = self.actor.sample(state)\n        \n        q1_new, q2_new = self.critic(state, new_action)\n        q_new = torch.min(q1_new, q2_new)\n        \n        # We want to maximize (Q - alpha * log_prob), so we minimize -(Q - alpha * log_prob)\n        actor_loss = (self.alpha * log_prob - q_new).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # -------------------------------------\n        # 3. Soft Update Target Networks\n        # -------------------------------------\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(self.conf.tau * param.data + (1 - self.conf.tau) * target_param.data)\n\n        return critic_loss.item(), actor_loss.item()\n\n    def save(self, filename):\n        torch.save(self.critic.state_dict(), filename + \"_critic.pth\")\n        torch.save(self.actor.state_dict(), filename + \"_actor.pth\")\n\n# ==========================================\n# 6. EXPERT HEURISTIC AGENT\n# ==========================================\nclass CarRacingHeuristic:\n    def __init__(self):\n        self.target_speed = 0.1 \n        self.k_p = 1.8  # Proportional gain for steering\n\n    def act(self, state):\n        if isinstance(state, torch.Tensor):\n            state = state.cpu().numpy()\n\n        img = np.transpose(state, (1, 2, 0))\n        crop = img[60:78, :, :]\n        r, g, b = crop[:, :, 0], crop[:, :, 1], crop[:, :, 2]\n        is_road = (np.abs(r - g) < 15) & (np.abs(g - b) < 15) & (g > 60)\n        road_pixels = np.argwhere(is_road)\n        \n        if len(road_pixels) > 0:\n            target_x = np.mean(road_pixels[:, 1])\n            error = (target_x - 48.0) / 48.0\n            steering = np.clip(error * self.k_p, -1.0, 1.0)\n            gas = 0.05 if abs(steering) > 0.3 else 0.2\n            brake = 0.0\n        else:\n            steering = 0.0\n            gas = 0.0\n            brake = 0.1\n\n        return np.array([steering, gas, brake], dtype=np.float32)\n\n# ==========================================\n# 7. Helpers: Evaluation & Recording\n# ==========================================\ndef evaluate_and_record(policy, env_id, seed, step, run_name, video_folder=\"videos\"):\n    run_video_folder = os.path.join(video_folder, run_name)\n    os.makedirs(run_video_folder, exist_ok=True)\n    \n    eval_env = gym.make(env_id, continuous=True, render_mode=\"rgb_array\")\n    eval_env = ImageTransposeWrapper(eval_env)\n    \n    video_prefix = f\"step-{step}\"\n    \n    eval_env = gym.wrappers.RecordVideo(\n        eval_env, \n        video_folder=run_video_folder, \n        name_prefix=video_prefix,\n        episode_trigger=lambda x: True, \n        disable_logger=True\n    )\n    \n    rewards = []\n    video_path = None\n    \n    for i in range(1): \n        state, _ = eval_env.reset(seed=seed + 100 + i)\n        terminated, truncated = False, False\n        episode_reward = 0\n        \n        while not (terminated or truncated):\n            # Pass evaluate=True to use deterministic action (mean)\n            action = policy.select_action(np.array(state), evaluate=True)\n            state, reward, terminated, truncated, _ = eval_env.step(action)\n            episode_reward += reward\n            \n        rewards.append(episode_reward)\n    \n    eval_env.close()\n    \n    expected_filename = f\"{video_prefix}-episode-0.mp4\"\n    expected_path = os.path.join(run_video_folder, expected_filename)\n    if os.path.exists(expected_path):\n        video_path = expected_path\n        \n    return rewards, video_path\n\n# ==========================================\n# 8. Main Training Loop\n# ==========================================\ndef run_training():\n    conf = SACConfig()\n    \n    wandb.init(\n        project=conf.project_name,\n        name=conf.run_name,\n        config=asdict(conf),\n        monitor_gym=False,\n        save_code=True\n    )\n\n    env = gym.make(conf.env_id, continuous=True)\n    env = ImageTransposeWrapper(env)\n    \n    env.action_space.seed(conf.seed)\n    torch.manual_seed(conf.seed)\n    np.random.seed(conf.seed)\n\n    state_shape = env.observation_space.shape\n    action_dim = env.action_space.shape[0]\n    max_action = float(env.action_space.high[0])\n\n    replay_buffer = ReplayBuffer(state_shape, action_dim, conf.buffer_size)\n    \n    # Initialize SAC instead of TD3\n    policy = SAC(action_dim, max_action, conf)\n    \n    # Initialize heuristic expert\n    expert = CarRacingHeuristic()\n\n    state, _ = env.reset(seed=conf.seed)\n    episode_reward = 0\n    episode_timesteps = 0\n    episode_num = 0\n\n    print(f\"---------------------------------------\")\n    print(f\"Starting Training: {conf.env_id} | SAC | Seed: {conf.seed}\")\n    print(f\"Observation Shape: {state_shape} | Action Dim: {action_dim}\")\n    print(f\"Warmup (Expert) Steps: {conf.learning_starts}\")\n    print(f\"Alpha (Entropy Coeff): {conf.alpha}\")\n    print(f\"---------------------------------------\")\n\n    for t in range(int(conf.total_timesteps)):\n        episode_timesteps += 1\n\n        # === 1. Warmup Phase (Expert Data Collection) ===\n        if t < conf.learning_starts:\n            action = expert.act(state)\n            \n            # Robustness noise for expert data\n            action[0] += np.random.normal(0, 0.1)\n            action[0] = np.clip(action[0], -1.0, 1.0)\n            action[1] = np.clip(action[1], 0.0, 1.0)\n            action[2] = np.clip(action[2], 0.0, 1.0)\n\n        # === 2. Training Phase (SAC Policy) ===\n        else:\n            # SAC select_action already handles sampling and exploration\n            action = policy.select_action(np.array(state), evaluate=False)\n            \n            # Enforce CarRacing specific constraints\n            # (Note: Actor outputs tanh [-1, 1], but Gas/Brake need [0, 1])\n            action[1] = max(action[1], 0.0) \n            action[2] = max(action[2], 0.0)\n\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        done = terminated or truncated\n        \n        replay_buffer.add(state, action, next_state, reward, float(terminated))\n        state = next_state\n        episode_reward += reward\n\n        # Train Policy\n        if t >= conf.learning_starts:\n            critic_loss, actor_loss = policy.train(replay_buffer)\n            \n            if t % 100 == 0:\n                wandb.log({\n                    \"train/critic_loss\": critic_loss,\n                    \"train/actor_loss\": actor_loss\n                }, step=t)\n\n        if done:\n            wandb.log({\n                \"train/episode_reward\": episode_reward,\n                \"train/episode_length\": episode_timesteps\n            }, step=t)\n            \n            print(f\"Step {t} | Episode {episode_num} | Reward: {episode_reward:.2f}\")\n            \n            state, _ = env.reset()\n            episode_reward = 0\n            episode_timesteps = 0\n            episode_num += 1\n\n        # Evaluation\n        if (t + 1) % conf.eval_freq == 0:\n            print(f\"Evaluating at step {t+1}...\")\n            \n            eval_rewards, video_path = evaluate_and_record(\n                policy, \n                conf.env_id, \n                conf.seed, \n                step=t+1,\n                run_name=conf.run_name \n            )\n            \n            mean_score = np.mean(eval_rewards)\n            wandb.log({\"eval/mean_reward\": mean_score}, step=t)\n            wandb.log({\"eval/episode_reward\": eval_rewards[0]}, step=t)\n            \n            if video_path:\n                print(f\"Uploading video: {video_path}\")\n                wandb.log({\n                    \"eval/video\": wandb.Video(\n                        video_path, \n                        fps=30, \n                        format=\"mp4\", \n                        caption=f\"Eval Step {t+1} | Score: {mean_score:.2f}\"\n                    )\n                }, step=t)\n\n    print(\"Training Complete.\")\n    \n    if conf.save_model:\n        save_path = \"sac_model_artifacts\"\n        os.makedirs(save_path, exist_ok=True)\n        policy.save(os.path.join(save_path, \"sac_carracing\"))\n\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T22:17:28.635346Z","iopub.execute_input":"2025-12-10T22:17:28.636067Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Virtual display started successfully.\nLogged into WandB via Kaggle Secrets.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr><tr><td>train/actor_loss</td><td>▃▅▂▃▃▃▂▄▁▃▅▃▃▃▃▆▂▃█▅▃▁▂</td></tr><tr><td>train/critic_loss</td><td>█▅▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/episode_length</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>train/episode_reward</td><td>▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>-69.75268</td></tr><tr><td>train/actor_loss</td><td>-0.70371</td></tr><tr><td>train/critic_loss</td><td>0.43919</td></tr><tr><td>train/episode_length</td><td>1000</td></tr><tr><td>train/episode_reward</td><td>-43.54839</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sac_heuristic_start_b898451c</strong> at: <a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing-v1-5/runs/wo1zncf8' target=\"_blank\">https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing-v1-5/runs/wo1zncf8</a><br> View project at: <a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing-v1-5' target=\"_blank\">https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing-v1-5</a><br>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251210_221211-wo1zncf8/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251210_221734-dsup2kah</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16/runs/dsup2kah' target=\"_blank\">sac_car_8fe4f077</a></strong> to <a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16' target=\"_blank\">https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16/runs/dsup2kah' target=\"_blank\">https://wandb.ai/yousefyousefyousef335-cairo-university/sac-carracing16/runs/dsup2kah</a>"},"metadata":{}},{"name":"stdout","text":"---------------------------------------\nStarting Training: CarRacing-v3 | SAC | Seed: 42\nObservation Shape: (3, 96, 96) | Action Dim: 3\nWarmup (Expert) Steps: 20000\nAlpha (Entropy Coeff): 0.2\n---------------------------------------\nStep 999 | Episode 0 | Reward: 126.15\nStep 1999 | Episode 1 | Reward: 129.51\nStep 2999 | Episode 2 | Reward: 182.83\nStep 3999 | Episode 3 | Reward: 198.25\nStep 4999 | Episode 4 | Reward: 143.67\nStep 5999 | Episode 5 | Reward: 137.29\nStep 6999 | Episode 6 | Reward: 112.03\nStep 7999 | Episode 7 | Reward: 134.97\nStep 8999 | Episode 8 | Reward: 71.17\nStep 9999 | Episode 9 | Reward: 228.47\nEvaluating at step 10000...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /kaggle/working/videos/sac_car_8fe4f077 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n  logger.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-10000-episode-0.mp4\nStep 10999 | Episode 10 | Reward: 75.32\nStep 11999 | Episode 11 | Reward: 169.78\nStep 12999 | Episode 12 | Reward: 98.05\nStep 13999 | Episode 13 | Reward: 171.13\nStep 14999 | Episode 14 | Reward: 190.32\nStep 15999 | Episode 15 | Reward: 64.84\nStep 16999 | Episode 16 | Reward: 205.92\nStep 17999 | Episode 17 | Reward: 129.17\nStep 18999 | Episode 18 | Reward: 206.86\nStep 19999 | Episode 19 | Reward: 159.40\nEvaluating at step 20000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-20000-episode-0.mp4\nStep 20999 | Episode 20 | Reward: -49.82\nStep 21999 | Episode 21 | Reward: -74.03\nStep 22999 | Episode 22 | Reward: -79.24\nStep 23999 | Episode 23 | Reward: -78.57\nStep 24999 | Episode 24 | Reward: -76.43\nStep 25999 | Episode 25 | Reward: -52.38\nStep 26999 | Episode 26 | Reward: -29.01\nStep 27999 | Episode 27 | Reward: -23.33\nStep 28999 | Episode 28 | Reward: -52.22\nStep 29999 | Episode 29 | Reward: -22.26\nEvaluating at step 30000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-30000-episode-0.mp4\nStep 30999 | Episode 30 | Reward: 48.41\nStep 31999 | Episode 31 | Reward: 9.63\nStep 32999 | Episode 32 | Reward: 28.30\nStep 33999 | Episode 33 | Reward: 112.31\nStep 34999 | Episode 34 | Reward: 166.44\nStep 35999 | Episode 35 | Reward: 175.00\nStep 36999 | Episode 36 | Reward: 155.89\nStep 37999 | Episode 37 | Reward: 13.14\nStep 38999 | Episode 38 | Reward: 157.73\nStep 39999 | Episode 39 | Reward: 56.36\nEvaluating at step 40000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-40000-episode-0.mp4\nStep 40999 | Episode 40 | Reward: 257.86\nStep 41999 | Episode 41 | Reward: 224.23\nStep 42999 | Episode 42 | Reward: -17.76\nStep 43999 | Episode 43 | Reward: 239.22\nStep 44999 | Episode 44 | Reward: 177.58\nStep 45999 | Episode 45 | Reward: 314.01\nStep 46999 | Episode 46 | Reward: 176.22\nStep 47999 | Episode 47 | Reward: 59.85\nStep 48999 | Episode 48 | Reward: 86.05\nStep 49999 | Episode 49 | Reward: 226.80\nEvaluating at step 50000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-50000-episode-0.mp4\nStep 50999 | Episode 50 | Reward: 246.67\nStep 51999 | Episode 51 | Reward: 273.29\nStep 52999 | Episode 52 | Reward: 318.18\nStep 53999 | Episode 53 | Reward: 56.55\nStep 54999 | Episode 54 | Reward: 208.51\nStep 55999 | Episode 55 | Reward: 49.84\nStep 56999 | Episode 56 | Reward: 65.32\nStep 57999 | Episode 57 | Reward: 349.12\nStep 58999 | Episode 58 | Reward: 347.37\nStep 59999 | Episode 59 | Reward: 456.86\nEvaluating at step 60000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-60000-episode-0.mp4\nStep 60999 | Episode 60 | Reward: 110.00\nStep 61999 | Episode 61 | Reward: 391.35\nStep 62715 | Episode 62 | Reward: 410.88\nStep 63715 | Episode 63 | Reward: 186.18\nStep 64715 | Episode 64 | Reward: 102.80\nStep 65715 | Episode 65 | Reward: 223.62\nStep 66715 | Episode 66 | Reward: 227.40\nStep 67715 | Episode 67 | Reward: 121.88\nStep 68715 | Episode 68 | Reward: 403.40\nStep 69715 | Episode 69 | Reward: 455.89\nEvaluating at step 70000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-70000-episode-0.mp4\nStep 70715 | Episode 70 | Reward: 334.78\nStep 71715 | Episode 71 | Reward: 297.16\nStep 72715 | Episode 72 | Reward: 151.68\nStep 73715 | Episode 73 | Reward: 56.55\nStep 74715 | Episode 74 | Reward: 46.76\nStep 75715 | Episode 75 | Reward: 308.09\nStep 76715 | Episode 76 | Reward: 113.74\nStep 77715 | Episode 77 | Reward: 389.96\nStep 78715 | Episode 78 | Reward: 254.72\nStep 79715 | Episode 79 | Reward: 237.54\nEvaluating at step 80000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-80000-episode-0.mp4\nStep 80715 | Episode 80 | Reward: 440.37\nStep 81715 | Episode 81 | Reward: 312.64\nStep 82715 | Episode 82 | Reward: 127.85\nStep 83715 | Episode 83 | Reward: 440.00\nStep 84715 | Episode 84 | Reward: 197.87\nStep 85715 | Episode 85 | Reward: -39.94\nStep 86715 | Episode 86 | Reward: 193.55\nStep 87715 | Episode 87 | Reward: 238.52\nStep 88715 | Episode 88 | Reward: -30.31\nStep 89715 | Episode 89 | Reward: 177.78\nEvaluating at step 90000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-90000-episode-0.mp4\nStep 90715 | Episode 90 | Reward: 497.60\nStep 91715 | Episode 91 | Reward: 324.75\nStep 92715 | Episode 92 | Reward: 283.05\nStep 93715 | Episode 93 | Reward: 258.26\nStep 94715 | Episode 94 | Reward: 200.00\nStep 95715 | Episode 95 | Reward: 341.38\nStep 96715 | Episode 96 | Reward: 401.72\nStep 97715 | Episode 97 | Reward: 312.59\nStep 98715 | Episode 98 | Reward: 163.89\nStep 99715 | Episode 99 | Reward: 511.30\nEvaluating at step 100000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-100000-episode-0.mp4\nStep 100715 | Episode 100 | Reward: 177.19\nStep 101715 | Episode 101 | Reward: 240.07\nStep 102715 | Episode 102 | Reward: 138.81\nStep 103715 | Episode 103 | Reward: 118.75\nStep 104715 | Episode 104 | Reward: 242.11\nStep 105715 | Episode 105 | Reward: 197.03\nStep 106715 | Episode 106 | Reward: 57.89\nStep 107715 | Episode 107 | Reward: 219.39\nStep 108715 | Episode 108 | Reward: 143.67\nStep 109715 | Episode 109 | Reward: 132.48\nEvaluating at step 110000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-110000-episode-0.mp4\nStep 110715 | Episode 110 | Reward: 182.76\nStep 111715 | Episode 111 | Reward: 354.20\nStep 112715 | Episode 112 | Reward: 116.03\nStep 113715 | Episode 113 | Reward: 206.67\nStep 114715 | Episode 114 | Reward: 177.03\nStep 115715 | Episode 115 | Reward: 300.68\nStep 116715 | Episode 116 | Reward: 422.44\nStep 117715 | Episode 117 | Reward: 360.38\nStep 118715 | Episode 118 | Reward: 175.64\nStep 119715 | Episode 119 | Reward: 31.27\nEvaluating at step 120000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-120000-episode-0.mp4\nStep 120715 | Episode 120 | Reward: -21.71\nStep 121715 | Episode 121 | Reward: 225.84\nStep 122715 | Episode 122 | Reward: 239.68\nStep 123715 | Episode 123 | Reward: 860.29\nStep 124715 | Episode 124 | Reward: 131.83\nStep 125715 | Episode 125 | Reward: 881.95\nStep 126715 | Episode 126 | Reward: 222.46\nStep 127715 | Episode 127 | Reward: 247.13\nStep 128715 | Episode 128 | Reward: 212.29\nStep 129715 | Episode 129 | Reward: 460.13\nEvaluating at step 130000...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n","output_type":"stream"},{"name":"stdout","text":"Uploading video: videos/sac_car_8fe4f077/step-130000-episode-0.mp4\nStep 130715 | Episode 130 | Reward: 183.28\nStep 131715 | Episode 131 | Reward: 851.39\nStep 132715 | Episode 132 | Reward: 893.03\nStep 133715 | Episode 133 | Reward: 631.71\nStep 134715 | Episode 134 | Reward: 235.88\nStep 135715 | Episode 135 | Reward: 210.34\n","output_type":"stream"}],"execution_count":null}]}